{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying spams in strong related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spamming_detected_all_dataset.csv\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('strength').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.text_lower[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIfHaveLinks(instancia):\n",
    "    instancia = re.sub(r\"http\\S+\", \"has_link\", instancia).lower()\n",
    "    instancia = re.sub(r\"https\\S+\", \"has_link\", instancia).lower()\n",
    "    if \"has_link\" in instancia:\n",
    "        return \"has_link\"\n",
    "    else:\n",
    "        return \"no_link\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_link'] = df.apply(lambda row: checkIfHaveLinks(row['text_lower']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('has_link').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['has_link'] == 'has_link']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote>TODO: usar o count vectorizer para contar os termos em geral e excluir destes fortemente relacionados os tweets que contém os termos não-relacionados (#NFT #Blockchain).<blockquote>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.read_csv(r'\\twitter-EDA\\tweets_preprocessing\\identify_number_of_posts_per_user.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.groupby('numberOfPosts')[['username']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = df_users[['tweet_id','numberOfPosts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = pd.merge(left=df, right=users, left_on='tweet_id', right_on='tweet_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.columns = ['datetime', 'tweet_id', 'processed_text', 'text_lower',\n",
    "       'textblob_sentiment', 'vader_sentiment', 'afinn_sentiment',\n",
    "       'textblob_score', 'vader_score', 'afinn_score', 'afinn_score_norm',\n",
    "       'strength', 'has_link', 'has_spam', 'user_presence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.to_csv('all_dataset_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificando termos comuns utilizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CountVectorizer to count the number of times each word occurs\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Create an array that shows the number of times specific terms appear in column text\n",
    "cv = CountVectorizer(ngram_range = (1,1))\n",
    "count_matrix = cv.fit_transform(df.processed_text)\n",
    "# create dataframe\n",
    "word_count = pd.DataFrame(cv.get_feature_names(), columns = ['term'])\n",
    "# sum the presence of terms and turn it into a list\n",
    "word_count[\"count\"] = count_matrix.sum(axis=0).tolist()[0]\n",
    "word_count = word_count.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "# most used words\n",
    "word_count[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count[word_count['term'] == 'token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salvando imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe_image as dfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strength = merge[['text_lower','strength', 'has_link', 'has_spam', 'user_presence']]\n",
    "dfi.export(\n",
    "    strength.head(20),\n",
    "    \"table_strength.png\",\n",
    "    table_conversion=\"matplotlib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = merge[['text_lower','has_link', 'has_spam', 'user_presence']]\n",
    "dfi.export(\n",
    "    spam.head(20),\n",
    "    \"table_spam.png\",\n",
    "    table_conversion=\"matplotlib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### spam isolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam = merge[['text_lower','user_presence']]\n",
    "dfi.export(\n",
    "    spam.head(20),\n",
    "    \"table_spam_user.png\",\n",
    "    table_conversion=\"matplotlib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(\n",
    "    only_norm.head(20),\n",
    "    \"table_afinn_norm.png\",\n",
    "    table_conversion=\"matplotlib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo pra detectar spamming terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spammingTerms = ['thil','nft','crypt','blockchain','asset','token']\n",
    "\n",
    "def checkIfHaveSpammingTerms(text):\n",
    "    for term in spammingTerms:\n",
    "        if term in text:\n",
    "            return \"has_spam\"\n",
    "    return \"no_spam\"\n",
    "\n",
    "df['has_spam'] = df.apply(lambda row: checkIfHaveSpammingTerms(row['text_lower']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo para detectar spam por links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def checkIfHaveLinks(text):\n",
    "    text = re.sub(r\"http\\S+\",\"has_link\",text).lower()\n",
    "    text = re.sub(r\"https\\S+\",\"has_link\",text).lower()\n",
    "    if \"has_link\" in text:\n",
    "        return \"has_link\"\n",
    "    else:\n",
    "        return \"no_link\"\n",
    "\n",
    "df['has_link'] = df.apply(lambda row: checkIfHaveLinks(row['text_lower']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algoritmo para detectar força da relação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedTopics = [\n",
    "    \"metaverse is\",\n",
    "    \"what metaverse\",\n",
    "    \"is metaverse\",\n",
    "    \"metaverses are\",\n",
    "    \"what metaverses\",\n",
    "    \"are metaverses\"\n",
    "]\n",
    "\n",
    "def checkIfRelated(text):\n",
    "    for term in relatedTopics:\n",
    "        if term in text:\n",
    "            return 'strong'\n",
    "    return 'weak'\n",
    "\n",
    "df['strength'] = df.apply(lambda row: checkIfRelated(row['text_lower']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count[word_count['term'].str.contains('token')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Identificando usuários spammers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge.groupby(['strength','has_spam','user_presence'])[['tweet_id']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Balanceando a base por categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.precision', 2)\n",
    "pd.set_option('display.float_format',  '{:,.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge[(merge['has_link'] == 'no_link') & (merge['has_spam'] == 'no_spam') & (merge['user_presence'] == '1 ou menos')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge['user_presence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_minor = merge[merge['strength'] == 'strong']\n",
    "dataset_major = merge[merge['strength'] == 'weak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fracos e Não spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_major[(dataset_major['has_link'] == 'no_link') & ((dataset_major['has_spam'] == 'no_spam'))].groupby(['textblob_sentiment'])[['tweet_id']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortes e Não spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_minor[(dataset_minor['has_link'] == 'no_link') & ((dataset_minor['has_spam'] == 'no_spam'))].groupby(['textblob_sentiment'])[['tweet_id']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fracos e Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_major[(dataset_major['has_link'] == 'has_link') & ((dataset_major['has_spam'] == 'has_spam'))].groupby(['textblob_sentiment'])[['tweet_id']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortes e Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_minor[(dataset_minor['has_link'] == 'has_link') & ((dataset_minor['has_spam'] == 'has_spam'))].groupby(['textblob_sentiment'])[['tweet_id']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_undersampled, y_undersampled = resample(merge[merge['strength'] == 'weak'], merge[merge['strength'] == 'strong'],\n",
    "                replace=True,\n",
    "                n_samples=merge[merge['strength'] == 'strong'].shape[0],\n",
    "                random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d54245c670ff253d1204420841bd04192ea0c38e456268d4f3a24d97e25d8999"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
