{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invite people for the party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe_image as dfi\n",
    "# dfi.export(df, 'dataframe.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"go_processed_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['datetime','tweet_id','text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.apply(lambda row: eval(row['tokenized']), axis=1)\n",
    "df['stemmed'] = df.apply(lambda row: eval(row['tokenized']), axis=1)\n",
    "df['lemmatized'] = df.apply(lambda row: eval(row['tokenized']), axis=1)\n",
    "df['processed_text'] = df.apply(lambda row: eval(row['tokenized']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação do NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wferreira.MPAC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "- remoção de caracteres indesejados.\n",
    "\n",
    "<blockquote><i>Many social media data consists of unstructured text \n",
    "data. Unstructured text data contains non-significant \n",
    "expressions. This \"dirty\" data needs to be cleaned for NLP \n",
    "work to be done effectively. In addition, non-English \n",
    "characters should be cleaned in tweet texts to carry out NLP \n",
    "work on English texts. Agrali e Aydin (2021).</i></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def removeLinks(instancia):\n",
    "    return re.sub(r\"http\\S+\", \"\", instancia).lower()\n",
    "\n",
    "def removePunctuation(instancia):\n",
    "    return instancia.replace('.','') \\\n",
    "        .replace(',','') \\\n",
    "        .replace(';','') \\\n",
    "        .replace('-','') \\\n",
    "        .replace(':','') \\\n",
    "        .replace('(','') \\\n",
    "        .replace(')','')\n",
    "\n",
    "def removeHTMLTags(instancia):\n",
    "    return BeautifulSoup(instancia, 'html.parser').get_text()\n",
    "\n",
    "def removeNonLettersAndNumbers(instancia):\n",
    "    return re.sub(r\"[^a-zA-Zà-úÀ-Ú0-9 ]\", \"\", instancia.lower())\n",
    "\n",
    "def removeSpacesFromCorners(instancia):\n",
    "    return instancia.strip(\" \").strip()\n",
    "\n",
    "def cleanText(instancia):\n",
    "    instancia = removeLinks(instancia)\n",
    "    instancia = removePunctuation(instancia)\n",
    "    instancia = removeHTMLTags(instancia)\n",
    "    instancia = removeNonLettersAndNumbers(instancia)\n",
    "    instancia = removeSpacesFromCorners(instancia)\n",
    "    return instancia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "- separa o texto por palavras\n",
    "\n",
    "<blockquote><i>Tokenization is one of the first text normalization \n",
    "operations to be implemented. The purpose of this step is to \n",
    "divide the text or paragraphs into smaller sections. In this way, \n",
    "more accurate transactions and analyzes are made. \n",
    "Tokenization can be used in two different ways, either word\u0002based or sentence based. In this study, a word-based \n",
    "tokenization process will be applied. Tokenization has been \n",
    "applied to the texts in the \"clean_text\" column of the dataset. \n",
    "The words in the text for each line formed the new \"tokenized\" \n",
    "column as a list. Agrali e Aydin (2021).<i></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(instancia):\n",
    "    '''\n",
    "    Recebe o texto e retorna uma lista de tokens.\n",
    "    '''\n",
    "    return nltk.tokenize.TweetTokenizer().tokenize(instancia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "- É uma técnica de reduzir uma palavra ao seu radical, mesmo que seu radical não seja válido no seu idioma, removendo prefixos e sufixos de uma palavra.\n",
    "\n",
    "<blockquote><i>Stemming is applied to remove the inflections (prefix or \n",
    "suffix) of words. Words that have the same meaning and \n",
    "spelling are evaluated as different words by taking a prefix or \n",
    "a suffix. Stemming process is used to prevent this. After the \n",
    "tokenization process, the words kept as a list will be converted \n",
    "into root words. Agrali e Aydin (2021).</i></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\wferreira.MPAC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('rslp')\n",
    "def stemText(instancia):\n",
    "    '''\n",
    "    Recebe uma lista de termos e retorna uma lista stemada.\n",
    "    '''\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    words = []\n",
    "    for w in instancia:\n",
    "        words.append(stemmer.stem(w))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "- <p>Reduz as palavras flexionadas adequadamente, garantindo que a palavra raiz pertença ao idioma. Determinando assim a palavra que representa seu lema.<p>\n",
    "- <p>Por exemplo: <strong>\"execuções\"</strong> e <strong>\"executar\"</strong> são formas da palavra <strong>execução</strong>, portanto <strong>execução</strong> é o lema da palavra.</p>\n",
    "\n",
    "<blockquote><i>Lemmatization is appliedas stemming can sometimes fail \n",
    "to find root words. Lemmatization, which considers the \n",
    "morphological analysis of words and appropriately separates \n",
    "the meaningful word into its roots, can be used as an \n",
    "alternative. Lemmatization is a very important method to find \n",
    "the smallest root form of a word. In this way, each word \n",
    "becomes able to represent itself. Agrali e Aydin (2021).</i></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lematizador = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizeText(instancia):\n",
    "    '''\n",
    "    Recebe uma lista de termos e retorna uma lista lematizada.\n",
    "    '''\n",
    "    words = []\n",
    "    for word in instancia:\n",
    "        words.append(lematizador.lemmatize(word))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metavers'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize('metavers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hater'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lematizador.lemmatize('haters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords\n",
    "\n",
    "- Palavras que não são relevantes na sentença\n",
    "\n",
    "<blockquote><i>Remove stopwordsstep is applied afterwards. While \n",
    "creating sentences, words that do not mean anything in terms \n",
    "of emotions and meanings are used. These words are the most \n",
    "common words in a language (such as “the”, “a”, “in”), which \n",
    "are usually helpful in sentence construction. These words are \n",
    "called stopwords. There are different stopwords for each \n",
    "language. In this study, English stopwords are discussed. \n",
    "Removing these words from the sentence will not have any \n",
    "effect in terms of sentiment analysis in the sentence. Agrali e Aydin (2021).</i></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(instancia):\n",
    "    '''\n",
    "    Recebe uma lista de termos e retorna uma lista\n",
    "    com stopwords removidas.\n",
    "    '''\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = [i for i in instancia if not i in stopwords]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(filename, data):    \n",
    "    data.to_csv('{}.csv'.format(filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingText(data, filename):\n",
    "    data['clean_text'] = data.progress_apply(lambda row: cleanText(row['text']), axis=1)\n",
    "    data['tokenized'] = data.progress_apply(lambda row: tokenizeText(row['clean_text']), axis=1)\n",
    "    data['stemmed'] = data.progress_apply(lambda row: stemText(row['tokenized']), axis=1)\n",
    "    data['lemmatized'] = data.progress_apply(lambda row: lemmatizeText(row['stemmed']), axis=1)\n",
    "    data['processed_text'] = data.progress_apply(lambda row: removeStopWords(row['lemmatized']), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3667775/3667775 [05:46<00:00, 10590.49it/s]\n",
      "100%|██████████| 3667775/3667775 [12:15<00:00, 4984.38it/s]\n",
      "100%|██████████| 3667775/3667775 [2:02:16<00:00, 499.94it/s]   \n",
      "100%|██████████| 3667775/3667775 [56:15<00:00, 1086.68it/s]  \n",
      "100%|██████████| 3667775/3667775 [57:47<00:00, 1057.83it/s]  \n"
     ]
    }
   ],
   "source": [
    "df = preprocessingText(df, 'withemoji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-10-01 00:12:02+00:00</td>\n",
       "      <td>1443730389986590720</td>\n",
       "      <td>✨Join me this Sunday to celebrate 6 months sin...</td>\n",
       "      <td>join me this sunday to celebrate 6 months sinc...</td>\n",
       "      <td>[join, me, this, sunday, to, celebrate, 6, mon...</td>\n",
       "      <td>[join, me, thil, sunday, to, celebrat, 6, mont...</td>\n",
       "      <td>[join, me, thil, sunday, to, celebrat, 6, mont...</td>\n",
       "      <td>join thil sunday celebrat 6 month sinc genesil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-01 00:13:00+00:00</td>\n",
       "      <td>1443730633360953344</td>\n",
       "      <td>@top7ico @MXCfoundation @DeFinePlatform @YOPfi...</td>\n",
       "      <td>top7ico mxcfoundation defineplatform yopfi o3s...</td>\n",
       "      <td>[top, 7ico, mxcfoundation, defineplatform, yop...</td>\n",
       "      <td>[top, 7ic, mxcfoundation, defineplatform, yopf...</td>\n",
       "      <td>[top, 7ic, mxcfoundation, defineplatform, yopf...</td>\n",
       "      <td>top 7ic mxcfoundation defineplatform yopf o3sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-10-01 00:13:09+00:00</td>\n",
       "      <td>1443730669419446291</td>\n",
       "      <td>@MetaSpatial_io Good project go to the moon 🚀🚀...</td>\n",
       "      <td>metaspatialio good project go to the moon muha...</td>\n",
       "      <td>[metaspatialio, good, project, go, to, the, mo...</td>\n",
       "      <td>[metaspatiali, good, project, go, to, the, moo...</td>\n",
       "      <td>[metaspatiali, good, project, go, to, the, moo...</td>\n",
       "      <td>metaspatiali good project go moon muhamad 8065...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-10-01 00:13:18+00:00</td>\n",
       "      <td>1443730706409033728</td>\n",
       "      <td>@Metaverse_Yin I just take the hype as alpha lol</td>\n",
       "      <td>metaverseyin i just take the hype as alpha lol</td>\n",
       "      <td>[metaverseyin, i, just, take, the, hype, as, a...</td>\n",
       "      <td>[metaverseyin, i, just, tak, the, hyp, as, alp...</td>\n",
       "      <td>[metaverseyin, i, just, tak, the, hyp, a, alph...</td>\n",
       "      <td>metaverseyin tak hyp alph lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-10-01 00:13:56+00:00</td>\n",
       "      <td>1443730867797565442</td>\n",
       "      <td>@Metaverse_Yin Keep it coming ! Lmao</td>\n",
       "      <td>metaverseyin keep it coming  lmao</td>\n",
       "      <td>[metaverseyin, keep, it, coming, lmao]</td>\n",
       "      <td>[metaverseyin, keep, it, coming, lma]</td>\n",
       "      <td>[metaverseyin, keep, it, coming, lma]</td>\n",
       "      <td>metaverseyin keep coming lma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    datetime             tweet_id  \\\n",
       "0  2021-10-01 00:12:02+00:00  1443730389986590720   \n",
       "1  2021-10-01 00:13:00+00:00  1443730633360953344   \n",
       "2  2021-10-01 00:13:09+00:00  1443730669419446291   \n",
       "3  2021-10-01 00:13:18+00:00  1443730706409033728   \n",
       "4  2021-10-01 00:13:56+00:00  1443730867797565442   \n",
       "\n",
       "                                                text  \\\n",
       "0  ✨Join me this Sunday to celebrate 6 months sin...   \n",
       "1  @top7ico @MXCfoundation @DeFinePlatform @YOPfi...   \n",
       "2  @MetaSpatial_io Good project go to the moon 🚀🚀...   \n",
       "3   @Metaverse_Yin I just take the hype as alpha lol   \n",
       "4               @Metaverse_Yin Keep it coming ! Lmao   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0  join me this sunday to celebrate 6 months sinc...   \n",
       "1  top7ico mxcfoundation defineplatform yopfi o3s...   \n",
       "2  metaspatialio good project go to the moon muha...   \n",
       "3     metaverseyin i just take the hype as alpha lol   \n",
       "4                  metaverseyin keep it coming  lmao   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [join, me, this, sunday, to, celebrate, 6, mon...   \n",
       "1  [top, 7ico, mxcfoundation, defineplatform, yop...   \n",
       "2  [metaspatialio, good, project, go, to, the, mo...   \n",
       "3  [metaverseyin, i, just, take, the, hype, as, a...   \n",
       "4             [metaverseyin, keep, it, coming, lmao]   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [join, me, thil, sunday, to, celebrat, 6, mont...   \n",
       "1  [top, 7ic, mxcfoundation, defineplatform, yopf...   \n",
       "2  [metaspatiali, good, project, go, to, the, moo...   \n",
       "3  [metaverseyin, i, just, tak, the, hyp, as, alp...   \n",
       "4              [metaverseyin, keep, it, coming, lma]   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [join, me, thil, sunday, to, celebrat, 6, mont...   \n",
       "1  [top, 7ic, mxcfoundation, defineplatform, yopf...   \n",
       "2  [metaspatiali, good, project, go, to, the, moo...   \n",
       "3  [metaverseyin, i, just, tak, the, hyp, a, alph...   \n",
       "4              [metaverseyin, keep, it, coming, lma]   \n",
       "\n",
       "                                      processed_text  \n",
       "0  join thil sunday celebrat 6 month sinc genesil...  \n",
       "1  top 7ic mxcfoundation defineplatform yopf o3sw...  \n",
       "2  metaspatiali good project go moon muhamad 8065...  \n",
       "3                      metaverseyin tak hyp alph lol  \n",
       "4                       metaverseyin keep coming lma  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CountVectorizer for counting number of times each word occurs\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer module convert a collection of text documents to a matrix of token counts.\n",
    "<blockquote><i>If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. Pedregosa et al. (2011). scikit-learn.org</i></blockquote>\n",
    "\n",
    "https://scikit-learn.org/stable/about.html#citing-scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: DELETE NaN values in the processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete some missing values in the collection\n",
    "df = df[df.processed_text.isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix which shows the number of times specific terms appear on column text\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(df.processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the terms\n",
    "#cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of frequency of terms in each document\n",
    "count_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wferreira.MPAC\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metavers</td>\n",
       "      <td>3570138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nft</td>\n",
       "      <td>1104533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thil</td>\n",
       "      <td>734549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>project</td>\n",
       "      <td>573580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crypt</td>\n",
       "      <td>282796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007954</th>\n",
       "      <td>faysalnafim</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007955</th>\n",
       "      <td>faysalkhan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007956</th>\n",
       "      <td>faysalhossain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007957</th>\n",
       "      <td>faysalchaudary</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007958</th>\n",
       "      <td>úú</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2007959 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   term    count\n",
       "0              metavers  3570138\n",
       "1                   nft  1104533\n",
       "2                  thil   734549\n",
       "3               project   573580\n",
       "4                 crypt   282796\n",
       "...                 ...      ...\n",
       "2007954     faysalnafim        1\n",
       "2007955      faysalkhan        1\n",
       "2007956   faysalhossain        1\n",
       "2007957  faysalchaudary        1\n",
       "2007958              úú        1\n",
       "\n",
       "[2007959 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe\n",
    "word_count = pd.DataFrame(cv.get_feature_names(), columns = ['term'])\n",
    "# sum the presence of terms and turn it into a list\n",
    "word_count[\"count\"] = count_matrix.sum(axis=0).tolist()[0]\n",
    "word_count = word_count.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "# 200 palavras mais usadas\n",
    "word_count[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(scipy.sparse._csr.csr_matrix, (3667775, 2007959))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(count_matrix), count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas library\n",
    "import pandas as pd\n",
    "# importing matplotlib library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metavers</td>\n",
       "      <td>3570138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nft</td>\n",
       "      <td>1104533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thil</td>\n",
       "      <td>734549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>project</td>\n",
       "      <td>573580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crypt</td>\n",
       "      <td>282796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gam</td>\n",
       "      <td>274057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hav</td>\n",
       "      <td>256039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>new</td>\n",
       "      <td>255632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mor</td>\n",
       "      <td>210418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lik</td>\n",
       "      <td>199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>world</td>\n",
       "      <td>198865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gamef</td>\n",
       "      <td>198531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ha</td>\n",
       "      <td>191135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>one</td>\n",
       "      <td>186121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>met</td>\n",
       "      <td>183810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>first</td>\n",
       "      <td>181781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>futur</td>\n",
       "      <td>176416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>get</td>\n",
       "      <td>164965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>good</td>\n",
       "      <td>162047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>int</td>\n",
       "      <td>155693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>great</td>\n",
       "      <td>147891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>web</td>\n",
       "      <td>145404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>blockchain</td>\n",
       "      <td>142568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>play</td>\n",
       "      <td>139959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>nftcommunity</td>\n",
       "      <td>134730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>playtoearn</td>\n",
       "      <td>134179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>airdrop</td>\n",
       "      <td>131500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>te</td>\n",
       "      <td>123619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>im</td>\n",
       "      <td>122396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>best</td>\n",
       "      <td>120179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gaming</td>\n",
       "      <td>118988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>real</td>\n",
       "      <td>118625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>virtual</td>\n",
       "      <td>118557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>dont</td>\n",
       "      <td>118098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>peopl</td>\n",
       "      <td>117422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>join</td>\n",
       "      <td>117187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>going</td>\n",
       "      <td>117046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>bsc</td>\n",
       "      <td>115413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>see</td>\n",
       "      <td>113323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>p2e</td>\n",
       "      <td>112709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>mak</td>\n",
       "      <td>111896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>tim</td>\n",
       "      <td>107597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>token</td>\n",
       "      <td>104030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>think</td>\n",
       "      <td>103331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>som</td>\n",
       "      <td>102668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>wa</td>\n",
       "      <td>101838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>vr</td>\n",
       "      <td>101836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>facebook</td>\n",
       "      <td>100815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            term    count\n",
       "0       metavers  3570138\n",
       "1            nft  1104533\n",
       "2           thil   734549\n",
       "3        project   573580\n",
       "4          crypt   282796\n",
       "5            gam   274057\n",
       "6            hav   256039\n",
       "7            new   255632\n",
       "8            mor   210418\n",
       "9            lik   199900\n",
       "10         world   198865\n",
       "11         gamef   198531\n",
       "12            ha   191135\n",
       "13           one   186121\n",
       "14           met   183810\n",
       "15         first   181781\n",
       "16         futur   176416\n",
       "17           get   164965\n",
       "18          good   162047\n",
       "19           int   155693\n",
       "20         great   147891\n",
       "21           web   145404\n",
       "22    blockchain   142568\n",
       "23          play   139959\n",
       "24  nftcommunity   134730\n",
       "25    playtoearn   134179\n",
       "26       airdrop   131500\n",
       "27            te   123619\n",
       "28            im   122396\n",
       "29          best   120179\n",
       "30        gaming   118988\n",
       "31          real   118625\n",
       "32       virtual   118557\n",
       "33          dont   118098\n",
       "34         peopl   117422\n",
       "35          join   117187\n",
       "36         going   117046\n",
       "37           bsc   115413\n",
       "38           see   113323\n",
       "39           p2e   112709\n",
       "40           mak   111896\n",
       "41           tim   107597\n",
       "42         token   104030\n",
       "43         think   103331\n",
       "44           som   102668\n",
       "45            wa   101838\n",
       "46            vr   101836\n",
       "47      facebook   100815"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count[word_count['count'] > 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d54245c670ff253d1204420841bd04192ea0c38e456268d4f3a24d97e25d8999"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
