{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invite people for the party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataframe_image as dfi\n",
    "# dfi.export(df, 'dataframe.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"go_processed_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['datetime','tweet_id','text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix imported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.apply(lambda row: eval(row['tokenized']), axis=1)\n",
    "df['stemmed'] = df.apply(lambda row: eval(row['tokenized']), axis=1)\n",
    "df['lemmatized'] = df.apply(lambda row: eval(row['tokenized']), axis=1)\n",
    "df['processed_text'] = df.apply(lambda row: eval(row['tokenized']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação do NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "\n",
    "- remoção de caracteres indesejados.\n",
    "\n",
    "<blockquote><i>Many social media data consists of unstructured text \n",
    "data. Unstructured text data contains non-significant \n",
    "expressions. This \"dirty\" data needs to be cleaned for NLP \n",
    "work to be done effectively. In addition, non-English \n",
    "characters should be cleaned in tweet texts to carry out NLP \n",
    "work on English texts. Agrali e Aydin (2021).</i></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def removeLinks(instancia):\n",
    "    return re.sub(r\"http\\S+\", \"\", instancia).lower()\n",
    "\n",
    "def removePunctuation(instancia):\n",
    "    return instancia.replace('.','') \\\n",
    "        .replace(',','') \\\n",
    "        .replace(';','') \\\n",
    "        .replace('-','') \\\n",
    "        .replace(':','') \\\n",
    "        .replace('(','') \\\n",
    "        .replace(')','')\n",
    "\n",
    "def removeHTMLTags(instancia):\n",
    "    return BeautifulSoup(instancia, 'html.parser').get_text()\n",
    "\n",
    "def removeNonLettersAndNumbers(instancia):\n",
    "    return re.sub(r\"[^a-zA-Zà-úÀ-Ú0-9 ]\", \"\", instancia.lower())\n",
    "\n",
    "def removeSpacesFromCorners(instancia):\n",
    "    return instancia.strip(\" \").strip()\n",
    "\n",
    "def cleanText(instancia):\n",
    "    instancia = removeLinks(instancia)\n",
    "    instancia = removePunctuation(instancia)\n",
    "    instancia = removeHTMLTags(instancia)\n",
    "    instancia = removeNonLettersAndNumbers(instancia)\n",
    "    instancia = removeSpacesFromCorners(instancia)\n",
    "    return instancia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "- separa o texto por palavras\n",
    "\n",
    "<blockquote><i>Tokenization is one of the first text normalization \n",
    "operations to be implemented. The purpose of this step is to \n",
    "divide the text or paragraphs into smaller sections. In this way, \n",
    "more accurate transactions and analyzes are made. \n",
    "Tokenization can be used in two different ways, either word\u0002based or sentence based. In this study, a word-based \n",
    "tokenization process will be applied. Tokenization has been \n",
    "applied to the texts in the \"clean_text\" column of the dataset. \n",
    "The words in the text for each line formed the new \"tokenized\" \n",
    "column as a list. Agrali e Aydin (2021).<i></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(instancia):\n",
    "    '''\n",
    "    Recebe o texto e retorna uma lista de tokens.\n",
    "    '''\n",
    "    return nltk.tokenize.TweetTokenizer().tokenize(instancia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "- É uma técnica de reduzir uma palavra ao seu radical, mesmo que seu radical não seja válido no seu idioma, removendo prefixos e sufixos de uma palavra.\n",
    "\n",
    "<blockquote><i>Stemming is applied to remove the inflections (prefix or \n",
    "suffix) of words. Words that have the same meaning and \n",
    "spelling are evaluated as different words by taking a prefix or \n",
    "a suffix. Stemming process is used to prevent this. After the \n",
    "tokenization process, the words kept as a list will be converted \n",
    "into root words. Agrali e Aydin (2021).</i></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('rslp')\n",
    "def stemText(instancia):\n",
    "    '''\n",
    "    Recebe uma lista de termos e retorna uma lista stemada.\n",
    "    '''\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    words = []\n",
    "    for w in instancia:\n",
    "        words.append(stemmer.stem(w))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "- <p>Reduz as palavras flexionadas adequadamente, garantindo que a palavra raiz pertença ao idioma. Determinando assim a palavra que representa seu lema.<p>\n",
    "- <p>Por exemplo: <strong>\"execuções\"</strong> e <strong>\"executar\"</strong> são formas da palavra <strong>execução</strong>, portanto <strong>execução</strong> é o lema da palavra.</p>\n",
    "\n",
    "<blockquote><i>Lemmatization is appliedas stemming can sometimes fail \n",
    "to find root words. Lemmatization, which considers the \n",
    "morphological analysis of words and appropriately separates \n",
    "the meaningful word into its roots, can be used as an \n",
    "alternative. Lemmatization is a very important method to find \n",
    "the smallest root form of a word. In this way, each word \n",
    "becomes able to represent itself. Agrali e Aydin (2021).</i></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lematizador = WordNetLemmatizer()\n",
    "\n",
    "def lemmatizeText(instancia):\n",
    "    '''\n",
    "    Recebe uma lista de termos e retorna uma lista lematizada.\n",
    "    '''\n",
    "    words = []\n",
    "    for word in instancia:\n",
    "        words.append(lematizador.lemmatize(word))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WordNetLemmatizer().lemmatize('metavers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lematizador.lemmatize('haters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stopwords\n",
    "\n",
    "- Palavras que não são relevantes na sentença\n",
    "\n",
    "<blockquote><i>Remove stopwordsstep is applied afterwards. While \n",
    "creating sentences, words that do not mean anything in terms \n",
    "of emotions and meanings are used. These words are the most \n",
    "common words in a language (such as “the”, “a”, “in”), which \n",
    "are usually helpful in sentence construction. These words are \n",
    "called stopwords. There are different stopwords for each \n",
    "language. In this study, English stopwords are discussed. \n",
    "Removing these words from the sentence will not have any \n",
    "effect in terms of sentiment analysis in the sentence. Agrali e Aydin (2021).</i></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(instancia):\n",
    "    '''\n",
    "    Recebe uma lista de termos e retorna uma lista\n",
    "    com stopwords removidas.\n",
    "    '''\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    words = [i for i in instancia if not i in stopwords]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(filename, data):    \n",
    "    data.to_csv('{}.csv'.format(filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessingText(data, filename):\n",
    "    data['clean_text'] = data.progress_apply(lambda row: cleanText(row['text']), axis=1)\n",
    "    data['tokenized'] = data.progress_apply(lambda row: tokenizeText(row['clean_text']), axis=1)\n",
    "    data['stemmed'] = data.progress_apply(lambda row: stemText(row['tokenized']), axis=1)\n",
    "    data['lemmatized'] = data.progress_apply(lambda row: lemmatizeText(row['stemmed']), axis=1)\n",
    "    data['processed_text'] = data.progress_apply(lambda row: removeStopWords(row['lemmatized']), axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessingText(df, 'withemoji')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CountVectorizer for counting number of times each word occurs\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer module convert a collection of text documents to a matrix of token counts.\n",
    "<blockquote><i>If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data. Pedregosa et al. (2011). scikit-learn.org</i></blockquote>\n",
    "\n",
    "https://scikit-learn.org/stable/about.html#citing-scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: DELETE NaN values in the processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete some missing values in the collection\n",
    "df = df[df.processed_text.isnull() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix which shows the number of times specific terms appear on column text\n",
    "cv = CountVectorizer()\n",
    "count_matrix = cv.fit_transform(df.processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the terms\n",
    "#cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of frequency of terms in each document\n",
    "count_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "word_count = pd.DataFrame(cv.get_feature_names(), columns = ['term'])\n",
    "# sum the presence of terms and turn it into a list\n",
    "word_count[\"count\"] = count_matrix.sum(axis=0).tolist()[0]\n",
    "word_count = word_count.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "# 200 palavras mais usadas\n",
    "word_count[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(count_matrix), count_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pandas library\n",
    "import pandas as pd\n",
    "# importing matplotlib library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count[word_count['count'] > 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d54245c670ff253d1204420841bd04192ea0c38e456268d4f3a24d97e25d8999"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
